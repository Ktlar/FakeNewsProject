{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "561d1ae4",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7493f15f",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "004b8bb4",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f86f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "#cleaned_content = \"\"\n",
    "\n",
    "def clean_text(txt):\n",
    "    #lowercasing text\n",
    "    txt = txt.lower()\n",
    "    #Removing excess whitespace, tabs and new lines\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt)\n",
    "    #remove special characters\n",
    "    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
    "    #numbers replaced with \"<NUM>\"#\n",
    "    txt = re.sub(r\"\\d+\", \"<NUM>\", txt)\n",
    "    #Dates replaced with \"<DATE>\"#\n",
    "    txt = re.sub(r\"\\d{4}-\\d{2}-\\d{2}\", \"<DATE>\", txt)\n",
    "    #Emails replaces with \"<EMAIL>\"#\n",
    "    txt = re.sub(r\"\\S+@\\S+\", \"<EMAIL>\", txt)\n",
    "    #URLS replaces with \"<URL>\"\n",
    "    txt = re.sub(r\"http\\S+\", \"<URL>\", txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "#we want to apply the clean text to content column in the dataframe#\n",
    "df[\"clean_news1\"] = df[\"content\"].apply(clean_text)\n",
    "\n",
    "# Tokenize the text\n",
    "df['tokens'] = df['clean_news1'].apply(nltk.word_tokenize)\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "# Compute the size of the vocabulary before and after removing stopwords\n",
    "vocab_before = len(set([word for tokens in df['tokens'] for word in tokens]))\n",
    "df['tokens_no_stopwords'] = df['tokens'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "vocab_after = len(set([word for tokens in df['tokens_no_stopwords'] for word in tokens]))\n",
    "reduction_rate_stopwords = (vocab_before - vocab_after) / vocab_before\n",
    "\n",
    "# Apply stemming\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "df['tokens_stemmed'] = df['tokens_no_stopwords'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Compute the size of the vocabulary before and after stemming\n",
    "vocab_before = len(set([word for tokens in df['tokens_no_stopwords'] for word in tokens]))\n",
    "vocab_after = len(set([word for tokens in df['tokens_stemmed'] for word in tokens]))\n",
    "reduction_rate_stemming = (vocab_before - vocab_after) / vocab_before\n",
    "\n",
    "print(f\"Vocabulary size before removing stopwords: {vocab_before}\")\n",
    "print(f\"Vocabulary size after removing stopwords: {vocab_after}\")\n",
    "print(f\"Reduction rate after removing stopwords: {reduction_rate_stopwords:.2%}\")\n",
    "print(f\"Vocabulary size after stemming: {vocab_after}\")\n",
    "print(f\"Reduction rate after stemming: {reduction_rate_stemming:.2%}\")\n",
    "\n",
    "\n",
    "# Create a frequency distribution of words in the dataset\n",
    "all_words = [word for tokens in df['tokens_stemmed'] for word in tokens]\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "\n",
    "# Print the 100 most common words\n",
    "print(\"100 most common words in the data after removing stopwords and stemming:\")\n",
    "for word, frequency in freq_dist.most_common(100):\n",
    "    print(f\"{word}: {frequency}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9e59865",
   "metadata": {},
   "source": [
    "For task 1, we've used the libraries:\n",
    "\n",
    "re: Which stands for regular expressions, which we use for filtering text. Using it to remove excess whitespace, tabs, new lines, urls, emails, numbers and dates. This is done so that the data is easier to explore.\n",
    "\n",
    "pandas: Which is a libary which is used for reading and manipulating data, this is probably the most fundamental library we use for this task, as we use pandas to read the csv file.\n",
    "\n",
    "nltk: Which is a library useful for our purpose of tokenizing and stemming the data.\n",
    "\n",
    "csv:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
