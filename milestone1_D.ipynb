{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enhanced-association",
   "metadata": {},
   "source": [
    "# Gruppe 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-courtesy",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sacred-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import scrapy\n",
    "import requests as req\n",
    "import html5lib\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#for task 4\n",
    "import scrapy\n",
    "import requests as req\n",
    "import html5lib\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse, TextResponse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-liabilities",
   "metadata": {},
   "source": [
    "To solve our assignment a bunch of different libraries are necessary. These libraries are used to scrape, parse, clean, process and explore our data. The first and probably most important library is Pandas which is a library used for reading and manipulating data.\n",
    "\n",
    "The second library is re which uses regular expressions for filtering text. By doing this it removes thing such as the urls, emails and dates among other stuff in order to make our data easier to explore.\n",
    "\n",
    "The third library is requests which is used to send http request to websites in order to gather data. In this assignment it is used to get data from https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\n",
    "\n",
    "The forth library is nltk which is really useful for purposes such as to tokenize and stem our data. Simply it helps clean the date.\n",
    "\n",
    "The fifth library is counter which is used to help in the process of counting.\n",
    "\n",
    "The sixth library is BeautifulSoup which is used for extracting data from htlm files.\n",
    "\n",
    "The seventh library is matplotlib which is for plotting data that helps visualizing data patterns.\n",
    "\n",
    "The eight library is html5lib which is a library for parsing HTML.\n",
    "\n",
    "The ninth and last library is Scrapy which is an application for extracting data from the websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "posted-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell is for processing and cleaning the data, it includes filtering and stemming the content\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\").apply(\n",
    "    lambda x: x.astype(str).str.lower())\n",
    "cleaned_content = \"\"\n",
    "\n",
    "for elm in data[\"content\"]: #regular expression for filtering data\n",
    "    url = re.sub(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", \"<URL>\", elm)\n",
    "    email = re.sub(\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", \"<EMAIL>\", url)\n",
    "    dates = re.sub(\"([0-9]{2}-[0-9]{2}-[0-9]{4}|[0-9]{2}\\/[0-9]{2}\\/[0-9]{4}|[a-zA-Z,]+([,])? [0-9]{2}([,])? [0-9]{4})\", \"<DATE>\",email) \n",
    "    num = re.sub(\"\\d+\", \"<NUM>\", dates)\n",
    "    cleaned_data = re.sub('\\W+', \" \", num)\n",
    "    cleaned_content = cleaned_data + cleaned_content \n",
    "   \n",
    "tokens = nltk.word_tokenize(cleaned_content)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_list = [] \n",
    "for n in tokens:\n",
    "    if n not in stop_words:\n",
    "        filtered_list.append(n)\n",
    "#tokenizing\n",
    "           \n",
    "ps = PorterStemmer()\n",
    "Stemmed_data = [] \n",
    "for i in filtered_list:\n",
    "  Stemmed_data.append(ps.stem(i))\n",
    "#stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "increasing-structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency of words in all articles is [('num', 2778), ('one', 482), ('like', 414), ('time', 398), ('peopl', 391), ('url', 390), ('state', 372), ('trump', 355), ('year', 342), ('use', 326), ('would', 325), ('market', 301), ('us', 288), ('also', 276), ('new', 272), ('make', 265), ('blockchain', 260), ('think', 239), ('go', 238), ('thing', 235), ('said', 232), ('govern', 232), ('way', 229), ('report', 228), ('mani', 227), ('even', 217), ('nation', 215), ('take', 212), ('next', 212), ('presid', 211), ('get', 211), ('see', 203), ('two', 201), ('could', 200), ('work', 196), ('say', 192), ('day', 190), ('world', 190), ('american', 189), ('stock', 189), ('life', 189), ('first', 187), ('need', 186), ('right', 185), ('u', 180), ('power', 178), ('may', 174), ('well', 172), ('want', 169), ('look', 164), ('countri', 158), ('help', 157), ('come', 156), ('feel', 156), ('obama', 155), ('war', 155), ('call', 154), ('includ', 153), ('back', 152), ('fact', 151), ('person', 151), ('bitcoin', 151), ('good', 147), ('bank', 147), ('much', 145), ('sourc', 144), ('search', 143), ('headlin', 142), ('long', 137), ('support', 136), ('polici', 135), ('clinton', 134), ('law', 134), ('point', 132), ('part', 132), ('exceed', 131), ('last', 128), ('know', 127), ('system', 125), ('live', 124), ('secur', 124), ('russia', 124), ('polit', 123), ('follow', 123), ('money', 122), ('research', 121), ('free', 120), ('realli', 120), ('continu', 120), ('product', 119), ('chang', 119), ('question', 118), ('compani', 117), ('end', 117), ('gener', 116), ('made', 116), ('industri', 116), ('global', 115), ('news', 114), ('million', 114)]\n"
     ]
    }
   ],
   "source": [
    "Most_common_words = Counter(\" \".join(Stemmed_data).split()).most_common(100)\n",
    "counter = Counter(Stemmed_data) \n",
    "\n",
    "#print(cleaned_content) #delete # to view a cleaned version of the full data set \n",
    "print(\"Highest frequency of words in all articles is\", Most_common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-livestock",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-april",
   "metadata": {},
   "source": [
    "In this task, we are assigned to make at least three non-trivial observations/discoveries about the data extracted. The observartions that we have made mostly reflect themselves on the rising intense political athmosphere in the United States. This semantic field of American politics has these three observations connected to it:\n",
    "1- The word \"Russia\" appears more than 100 times which is understandable in relation to Trump's presidential victory and the numerous news articles etc, both factual and unfactual, that have flourished around Trump's suspicious so called \"cooperation\" with Kremlin. This case has been one of the few cases which has divided the US between the Trump supporters and the left which accuses Trump of treason for the above reason.\n",
    "\n",
    "2- Another remarkable observation is appearance of the words \"Countri\"(countries?) and \"nation\" which in recent years have filled much of Trump's rhetoric, as his audience is dominated by \"patriotic\" supporters. This sovereignty trend leads to fake news surrounding false threats from several ethnicities in the US, as well as conspiracy theories such as the existence of a deep-state, and these misfortunes are abused by both sides of the political compass.\n",
    "\n",
    "3- The more international matters have also made their appearances in the news stories as Trump is known, by FOX news especially,for being one of the few presidents in modern times to not have started an unnecessary war in the Middle East unlike former president Obama. This leads to more news services demonizing Obama and make Trump a holy saint, or vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-longitude",
   "metadata": {},
   "source": [
    "Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "framed-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGHIJ\n",
      "                                 Overskrift på artikel  \\\n",
      "0                  G20 protests: Inside a labour march   \n",
      "1                  G8 leaders set new emissions target   \n",
      "2             G8 members release statement on Zimbabwe   \n",
      "3     G8 reaches deal on African aid for health issues   \n",
      "4    G8 Summit debates Middle-east crisis, WTO trad...   \n",
      "..                                                 ...   \n",
      "195       Guantanamo prosecutors call trials a \"fraud\"   \n",
      "196  Guatemala arrests Twitter user for inciting fi...   \n",
      "197  Guatemala nominates Rigoberta Menchú for OAS S...   \n",
      "198  Guatemalan government may suspend liberties un...   \n",
      "199     Guatemalan minister killed in helicopter crash   \n",
      "\n",
      "                                      Link til artikel  \\\n",
      "0    https://en.wikinews.org//wiki/G20_protests:_In...   \n",
      "1    https://en.wikinews.org//wiki/G8_leaders_set_n...   \n",
      "2    https://en.wikinews.org//wiki/G8_members_relea...   \n",
      "3    https://en.wikinews.org//wiki/G8_reaches_deal_...   \n",
      "4    https://en.wikinews.org//wiki/G8_Summit_debate...   \n",
      "..                                                 ...   \n",
      "195  https://en.wikinews.org//wiki/Guantanamo_prose...   \n",
      "196  https://en.wikinews.org//wiki/Guatemala_arrest...   \n",
      "197  https://en.wikinews.org//wiki/Guatemala_nomina...   \n",
      "198  https://en.wikinews.org//wiki/Guatemalan_gover...   \n",
      "199  https://en.wikinews.org//wiki/Guatemalan_minis...   \n",
      "\n",
      "    Udgivelsesdato på artikel  \n",
      "0                  2009-04-03  \n",
      "1                  2009-07-10  \n",
      "2                  2008-07-09  \n",
      "3                  2007-06-08  \n",
      "4                  2006-07-17  \n",
      "..                        ...  \n",
      "195                2005-08-01  \n",
      "196                2009-05-14  \n",
      "197                2005-01-10  \n",
      "198                2009-05-10  \n",
      "199                2008-06-28  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the letters the group has to work with\n",
    "\n",
    "print(\"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[18%23:18%23 + 10])\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"news\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "                'https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=A'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'news-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)\n",
    "\n",
    "\n",
    "link = \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=G\"\n",
    "\n",
    "\n",
    "wiki = req.get(link).text\n",
    "soup = BeautifulSoup(wiki, 'html.parser')\n",
    "content = soup.find_all('div', attrs={'class': 'mw-category'})\n",
    "li = content[1].find_all('li')\n",
    " \n",
    "#Creating an empty list so it can be filled with links\n",
    "\n",
    "liste_over_links = []\n",
    "\n",
    "\n",
    "#Iterating through all links and putting necessary things in the list\n",
    "\n",
    "for item in li:\n",
    "    link = item.find('a')\n",
    "    link = 'https://en.wikinews.org/' + link['href']\n",
    "    liste_over_links.append(link)\n",
    "\n",
    "\n",
    "#List with headers to be put into dataset\n",
    "\n",
    "liste_over_overskrifter = []\n",
    "\n",
    "for item in li:\n",
    "    overskrift = item.find('a')\n",
    "    overskrift = overskrift['title']\n",
    "    liste_over_overskrifter.append(overskrift)\n",
    "    \n",
    "\n",
    "#Dates of articles when published\n",
    "\n",
    "liste_over_dato = []\n",
    "\n",
    "for link in liste_over_links:\n",
    "    wiki = req.get(link).text\n",
    "    soup = BeautifulSoup(wiki, 'html.parser')\n",
    "    content = soup.find('span', attrs={'class': 'value-title'})\n",
    "    \n",
    "    #some dates gave a none value, thus, we made an empty one\n",
    "    if content is None:\n",
    "        content = \"Empty\"\n",
    "    else:\n",
    "        content = content['title']\n",
    "\n",
    "    liste_over_dato.append(content)\n",
    "    \n",
    "\n",
    "#pandas dataset with the 3 lists\n",
    "data = {'Overskrift på artikel': liste_over_overskrifter, 'Link til artikel': liste_over_links, 'Udgivelsesdato på artikel': liste_over_dato}\n",
    "data = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-bobby",
   "metadata": {},
   "source": [
    "Above is seen metadata/statistics for wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reverse-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_wikinews(letter_range):\n",
    "    # Initialize empty list to hold articles\n",
    "    articles = []\n",
    "    \n",
    "    # Loop over each letter in the range\n",
    "    for letter in letter_range:\n",
    "        # Construct URL for letter page\n",
    "        url = f\"https://en.wikinews.org/wiki/Category:Politics_and_conflicts?from={letter}\"\n",
    "        \n",
    "        # Send GET request to URL and get HTML response\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        \n",
    "        # Parse HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Find all article links on page\n",
    "        links = soup.find_all(\"div\", {\"class\": \"mw-category-generated\"})[0].find_all(\"a\")\n",
    "        \n",
    "        # Loop over each link and extract article information\n",
    "        for link in links:\n",
    "            # Get article title and URL\n",
    "            title = link.text\n",
    "            article_url = \"https://en.wikinews.org\" + link.get(\"href\")\n",
    "            \n",
    "            # Send GET request to article URL and get HTML response\n",
    "            response = requests.get(article_url)\n",
    "            html = response.text\n",
    "            \n",
    "            # Parse HTML using BeautifulSoup\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find article content and date\n",
    "            content_div = soup.find_all(\"div\", {\"class\": \"mw-parser-output\"})[0]\n",
    "            paragraphs = [p.text for p in content_div.find_all(\"p\")]\n",
    "            content = \"\\n\\n\".join(paragraphs)\n",
    "            date_str = soup.find_all(\"span\", {\"class\": \"published\"})[0].text\n",
    "            date = datetime.strptime(date_str, \"%A, %B %d, %Y\")\n",
    "            \n",
    "            # Append article information to list\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"url\": article_url,\n",
    "                \"date\": date,\n",
    "                \"content\": content\n",
    "            })\n",
    "    \n",
    "    # Return list of articles\n",
    "    return articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da350130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUVWZABCDE\n"
     ]
    }
   ],
   "source": [
    "print(\"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[18%23:18%23 + 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6461d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol=[T,U,V,W,Z,A,B,C,D,E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af9cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
